{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install janome 일본어 형태소 분석기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kti08\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kti08\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk download\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "view_log_train = pd.read_csv('./open/view_log.csv')\n",
    "article_info = pd.read_csv('./open/article_info.csv')\n",
    "submission = pd.read_csv('./open/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리\n",
    "article_info['userCountry'].fillna('Unknown', inplace=True)\n",
    "article_info['userRegion'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자-기사 행렬 생성\n",
    "user_article_matrix = view_log_train.groupby(['userID', 'articleID']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 로드\n",
    "stop_words_dict = {\n",
    "    'en': stopwords.words('english'),\n",
    "    'pt': stopwords.words('portuguese'),\n",
    "    'la': ['et', 'in', 'de'],\n",
    "    'es': stopwords.words('spanish')\n",
    "}\n",
    "\n",
    "# 일본어 불용어 직접 정의\n",
    "japanese_stop_words = ['これ', 'それ', 'あれ', 'この', 'その', 'あの', 'ここ', 'そこ', 'あそこ', 'こちら', 'どこ', 'だれ', 'なに', 'なん']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수 정의\n",
    "def preprocess_text(text, language):\n",
    "\n",
    "    # URL 제거\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    if language == 'en':\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    elif language == 'pt':\n",
    "        text = re.sub(r'[^a-zA-Z0-9áéíóúâêîôûãõçÇ\\s]', '', text)\n",
    "    elif language == 'la':\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    elif language == 'ja':\n",
    "        text = re.sub(r'[^\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF\\s]', '', text)\n",
    "    elif language == 'es':\n",
    "        text = re.sub(r'[^a-zA-Z0-9áéíóúñÑ\\s]', '', text)\n",
    "\n",
    "    # 소문자 변환\n",
    "    text = text.lower()\n",
    "\n",
    "    # 토큰화 및 불용어 제거\n",
    "    if language == 'ja':\n",
    "        tokenizer = Tokenizer()\n",
    "        tokens = [token.surface for token in tokenizer.tokenize(text)]\n",
    "        tokens = [token for token in tokens if token not in japanese_stop_words]\n",
    "\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stop_words = stop_words_dict.get(language, [])\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 기사에 대해 전처리 적용\n",
    "article_info['ProcessedContent'] = article_info.apply(lambda row: preprocess_text(row['Content'], row['Language']), axis=1)\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(article_info['ProcessedContent'])\n",
    "\n",
    "# 유사도 행렬 계산\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# 사용자 간 유사성 계산\n",
    "user_similarity = cosine_similarity(user_article_matrix)\n",
    "\n",
    "# 사용자 기반 추천 점수 계산\n",
    "user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "\n",
    "\n",
    "# 사용자별 콘텐츠 기반 추천 점수 계산\n",
    "content_based_scores = np.zeros(user_article_matrix.shape)\n",
    "\n",
    "# 전체 기사 수\n",
    "num_articles = len(article_info)\n",
    "\n",
    "for user_idx, user in enumerate(user_article_matrix.index):\n",
    "    user_viewed_articles = user_article_matrix.columns[user_article_matrix.loc[user] > 0]\n",
    "    if len(user_viewed_articles) > 0:\n",
    "        user_articles_idx = [list(user_article_matrix.columns).index(article) for article in user_viewed_articles]\n",
    "        user_content_scores = similarity_matrix[user_articles_idx].mean(axis=0)\n",
    "        # 사용자별 점수에 맞게 reshape\n",
    "        user_content_scores = user_content_scores[:user_article_matrix.shape[1]]\n",
    "        content_based_scores[user_idx] = user_content_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 협업 필터링 점수와 콘텐츠 기반 점수를 조합하여 최종 추천 점수 계산\n",
    "final_scores = 0.42 * user_predicted_scores + 0.58 * content_based_scores\n",
    "\n",
    "# 이미 조회한 기사 포함해서 추천\n",
    "recommendations = []\n",
    "for idx, user in enumerate(user_article_matrix.index):\n",
    "    sorted_indices = final_scores[idx].argsort()[::-1]\n",
    "    top5recommend = [article for article in user_article_matrix.columns[sorted_indices]][:5]\n",
    "    for article in top5recommend:\n",
    "        recommendations.append([user, article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame 생성 및 제출 파일 저장\n",
    "top_recommendations = pd.DataFrame(recommendations, columns=['userID', 'articleID'])\n",
    "submission['articleID'] = top_recommendations['articleID']\n",
    "submission.to_csv('hybrid_recommendation_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
